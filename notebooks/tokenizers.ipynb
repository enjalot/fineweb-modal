{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enjalot/code/fineweb-modal/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_tokenizers(text_samples):\n",
    "    \"\"\"\n",
    "    Compare tokenization results between BGE and Nomic tokenizers\n",
    "    \n",
    "    Args:\n",
    "        text_samples: List of text strings to compare tokenization\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comparison statistics and analysis results\n",
    "    \"\"\"\n",
    "    # Load both tokenizers\n",
    "    bge_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-base-en-v1.5\")\n",
    "    nomic_tokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/nomic-embed-text-v1.5\")\n",
    "    \n",
    "    results = {\n",
    "        \"vocabulary_sizes\": {\n",
    "            \"bge\": len(bge_tokenizer.vocab),\n",
    "            \"nomic\": len(nomic_tokenizer.vocab),\n",
    "        },\n",
    "        \"samples\": []\n",
    "    }\n",
    "    \n",
    "    # Compare tokenization for each sample\n",
    "    for text in text_samples:\n",
    "        bge_tokens = bge_tokenizer.tokenize(text)\n",
    "        nomic_tokens = nomic_tokenizer.tokenize(text)\n",
    "        \n",
    "        # Get token counts\n",
    "        bge_counts = Counter(bge_tokens)\n",
    "        nomic_counts = Counter(nomic_tokens)\n",
    "        \n",
    "        # Compare token sequences\n",
    "        sample_result = {\n",
    "            \"text\": text,\n",
    "            \"bge_tokens\": bge_tokens,\n",
    "            \"nomic_tokens\": nomic_tokens,\n",
    "            \"token_counts\": {\n",
    "                \"bge\": len(bge_tokens),\n",
    "                \"nomic\": len(nomic_tokens)\n",
    "            },\n",
    "            \"unique_tokens\": {\n",
    "                \"bge\": len(bge_counts),\n",
    "                \"nomic\": len(nomic_counts)\n",
    "            },\n",
    "            \"identical_tokenization\": bge_tokens == nomic_tokens\n",
    "        }\n",
    "        \n",
    "        results[\"samples\"].append(sample_result)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    identical_count = sum(1 for r in results[\"samples\"] if r[\"identical_tokenization\"])\n",
    "    results[\"overall_stats\"] = {\n",
    "        \"total_samples\": len(text_samples),\n",
    "        \"identical_tokenizations\": identical_count,\n",
    "        \"identical_percentage\": (identical_count / len(text_samples)) * 100 if text_samples else 0\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_comparison_report(results):\n",
    "    \"\"\"Print a formatted report of the tokenizer comparison results\"\"\"\n",
    "    print(\"Tokenizer Comparison Report\")\n",
    "    print(\"==========================\")\n",
    "    print(f\"\\nVocabulary Sizes:\")\n",
    "    print(f\"BGE: {results['vocabulary_sizes']['bge']:,} tokens\")\n",
    "    print(f\"Nomic: {results['vocabulary_sizes']['nomic']:,} tokens\")\n",
    "    \n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(f\"Total samples analyzed: {results['overall_stats']['total_samples']}\")\n",
    "    print(f\"Identical tokenizations: {results['overall_stats']['identical_tokenizations']}\")\n",
    "    print(f\"Percentage identical: {results['overall_stats']['identical_percentage']:.1f}%\")\n",
    "    \n",
    "    print(\"\\nDetailed Sample Analysis:\")\n",
    "    for i, sample in enumerate(results['samples'], 1):\n",
    "        print(f\"\\nSample {i}:\")\n",
    "        print(f\"Text: {sample['text']}\")\n",
    "        print(f\"BGE tokens ({sample['token_counts']['bge']}): {sample['bge_tokens']}\")\n",
    "        print(f\"Nomic tokens ({sample['token_counts']['nomic']}): {sample['nomic_tokens']}\")\n",
    "        print(f\"Identical: {sample['identical_tokenization']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Comparison Report\n",
      "==========================\n",
      "\n",
      "Vocabulary Sizes:\n",
      "BGE: 30,522 tokens\n",
      "Nomic: 30,522 tokens\n",
      "\n",
      "Overall Statistics:\n",
      "Total samples analyzed: 3\n",
      "Identical tokenizations: 3\n",
      "Percentage identical: 100.0%\n",
      "\n",
      "Detailed Sample Analysis:\n",
      "\n",
      "Sample 1:\n",
      "Text: This is a test sentence.\n",
      "BGE tokens (6): ['this', 'is', 'a', 'test', 'sentence', '.']\n",
      "Nomic tokens (6): ['this', 'is', 'a', 'test', 'sentence', '.']\n",
      "Identical: True\n",
      "\n",
      "Sample 2:\n",
      "Text: Machine learning models use different tokenization approaches.\n",
      "BGE tokens (9): ['machine', 'learning', 'models', 'use', 'different', 'token', '##ization', 'approaches', '.']\n",
      "Nomic tokens (9): ['machine', 'learning', 'models', 'use', 'different', 'token', '##ization', 'approaches', '.']\n",
      "Identical: True\n",
      "\n",
      "Sample 3:\n",
      "Text: Some текст with mixed 字符 and специальные characters!\n",
      "BGE tokens (24): ['some', 'т', '##е', '##к', '##с', '##т', 'with', 'mixed', '[UNK]', '[UNK]', 'and', 'с', '##п', '##е', '##ц', '##и', '##а', '##л', '##ь', '##н', '##ы', '##е', 'characters', '!']\n",
      "Nomic tokens (24): ['some', 'т', '##е', '##к', '##с', '##т', 'with', 'mixed', '[UNK]', '[UNK]', 'and', 'с', '##п', '##е', '##ц', '##и', '##а', '##л', '##ь', '##н', '##ы', '##е', 'characters', '!']\n",
      "Identical: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "sample_texts = [\n",
    "    \"This is a test sentence.\",\n",
    "    \"Machine learning models use different tokenization approaches.\",\n",
    "    \"Some текст with mixed 字符 and специальные characters!\",\n",
    "]\n",
    "\n",
    "results = compare_tokenizers(sample_texts)\n",
    "print_comparison_report(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-base-en-v1.5\")\n",
    "nomic_tokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/nomic-embed-text-v1.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='BAAI/bge-base-en-v1.5', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bge_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='nomic-ai/nomic-embed-text-v1.5', vocab_size=30522, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nomic_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
