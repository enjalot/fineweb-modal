{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# import tqdm\n",
    "from tqdm.notebook import tqdm  # Import the notebook version of tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import huggingface_hub\n",
    "from huggingface_hub import HfFileSystem\n",
    "hffs = HfFileSystem()\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", data_files=\"sample/10BT/*.parquet\", streaming=True, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = hffs.ls(\"datasets/HuggingFaceFW/fineweb-edu/sample/10BT\", detail=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/HuggingFaceFW/fineweb-edu/sample/10BT/000_00000.parquet',\n",
       " 'datasets/HuggingFaceFW/fineweb-edu/sample/10BT/001_00000.parquet',\n",
       " 'datasets/HuggingFaceFW/fineweb-edu/sample/10BT/002_00000.parquet',\n",
       " 'datasets/HuggingFaceFW/fineweb-edu/sample/10BT/003_00000.parquet',\n",
       " 'datasets/HuggingFaceFW/fineweb-edu/sample/10BT/004_00000.parquet',\n",
       " 'datasets/HuggingFaceFW/fineweb-edu/sample/10BT/005_00000.parquet',\n",
       " 'datasets/HuggingFaceFW/fineweb-edu/sample/10BT/006_00000.parquet',\n",
       " 'datasets/HuggingFaceFW/fineweb-edu/sample/10BT/007_00000.parquet',\n",
       " 'datasets/HuggingFaceFW/fineweb-edu/sample/10BT/008_00000.parquet',\n",
       " 'datasets/HuggingFaceFW/fineweb-edu/sample/10BT/009_00000.parquet',\n",
       " 'datasets/HuggingFaceFW/fineweb-edu/sample/10BT/010_00000.parquet',\n",
       " 'datasets/HuggingFaceFW/fineweb-edu/sample/10BT/011_00000.parquet',\n",
       " 'datasets/HuggingFaceFW/fineweb-edu/sample/10BT/012_00000.parquet',\n",
       " 'datasets/HuggingFaceFW/fineweb-edu/sample/10BT/013_00000.parquet']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet(\"hf://\" + files[0])\n",
    "df = pd.read_parquet(file.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_parquet(files[0].split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep_keys = [\"id\", \"url\", \"score\", \"dump\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", model_max_length=MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chunk(rows):\n",
    "#     texts = rows[\"text\"]\n",
    "#     chunks_index = []\n",
    "#     chunks_text = []\n",
    "#     chunks_tokens = []\n",
    "#     updated_token_counts = []\n",
    "\n",
    "#     # Assuming you have other properties in the rows that you want to retain\n",
    "#     keep = {key: [] for key in keep_keys}\n",
    "\n",
    "#     for index, text in enumerate(texts):\n",
    "#         tokens = tokenizer.encode(text)\n",
    "#         token_count = len(tokens)\n",
    "\n",
    "#         if token_count > MAX_TOKENS:\n",
    "#             overlap = int(MAX_TOKENS * 0.1)\n",
    "#             start_index = 0\n",
    "#             ci = 0\n",
    "#             while start_index < len(tokens):\n",
    "#                 end_index = min(start_index + MAX_TOKENS, len(tokens))\n",
    "#                 chunk = tokens[start_index:end_index]\n",
    "#                 chunks_index.append(ci)\n",
    "#                 chunks_tokens.append(chunk)\n",
    "#                 updated_token_counts.append(len(chunk))\n",
    "#                 chunks_text.append(tokenizer.decode(chunk))\n",
    "#                 # Copy other properties for each chunk\n",
    "#                 for key in keep:\n",
    "#                     keep[key].append(rows[key][index])\n",
    "#                 start_index += MAX_TOKENS - overlap\n",
    "#                 ci += 1\n",
    "#         else:\n",
    "#             chunks_index.append(0)\n",
    "#             chunks_text.append(text)\n",
    "#             chunks_tokens.append(tokens)\n",
    "#             updated_token_counts.append(token_count)\n",
    "#             # Copy other properties for non-chunked texts\n",
    "#             for key in keep:\n",
    "#                 keep[key].append(rows[key][index])\n",
    "\n",
    "#     keep[\"chunk_index\"] = chunks_index\n",
    "#     keep[\"chunk_text\"] = chunks_text\n",
    "#     keep[\"chunk_tokens\"] = chunks_tokens\n",
    "#     keep[\"chunk_token_count\"] = updated_token_counts\n",
    "#     return keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_row(row, tokenizer):\n",
    "    # print(\"ROW\", row)\n",
    "    MAX_TOKENS = 512\n",
    "    keep_keys = [\"id\", \"url\", \"score\", \"dump\"]\n",
    "    text = row[\"text\"]\n",
    "    chunks = []\n",
    "\n",
    "    tokens = tokenizer.encode(text)\n",
    "    token_count = len(tokens)\n",
    "    if token_count > MAX_TOKENS:\n",
    "        overlap = int(MAX_TOKENS * 0.1)\n",
    "        start_index = 0\n",
    "        ci = 0\n",
    "        while start_index < len(tokens):\n",
    "            end_index = min(start_index + MAX_TOKENS, len(tokens))\n",
    "            chunk = tokens[start_index:end_index]\n",
    "            chunks.append({\n",
    "                \"chunk_index\": ci,\n",
    "                \"chunk_text\": tokenizer.decode(chunk),\n",
    "                \"chunk_tokens\": chunk,\n",
    "                \"chunk_token_count\": len(chunk),\n",
    "                **{key: row[key] for key in keep_keys}\n",
    "            })\n",
    "            start_index += MAX_TOKENS - overlap\n",
    "            ci += 1\n",
    "    else:\n",
    "        chunks.append({\n",
    "            \"chunk_index\": 0,\n",
    "            \"chunk_text\": text,\n",
    "            \"chunk_tokens\": tokens,\n",
    "            \"chunk_token_count\": token_count,\n",
    "            **{key: row[key] for key in keep_keys}\n",
    "        })\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df):\n",
    "    chunks_list = []\n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        # Submit all rows to the executor\n",
    "        pbar = tqdm(total=len(df), desc=\"Processing Rows\")\n",
    "        \n",
    "        def process_batch(batch):\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", model_max_length=MAX_TOKENS)\n",
    "            batch_chunks = []\n",
    "            for row in batch:\n",
    "                row_chunks = chunk_row(row, tokenizer)\n",
    "                pbar.update(1)\n",
    "                batch_chunks.extend(row_chunks)\n",
    "            return batch_chunks\n",
    "\n",
    "\n",
    "        print(\"making batches\")\n",
    "        batch_size = 200  # Adjust batch size based on your needs\n",
    "        batches = [df.iloc[i:i + batch_size].to_dict(orient=\"records\") for i in range(0, len(df), batch_size)]\n",
    "        print(\"made batches\")\n",
    "        print(\"setting up futures\")\n",
    "        futures = [executor.submit(process_batch, batch) for batch in batches]\n",
    "        # futures = [executor.submit(chunk_row, row) for index, row in df.iterrows()]\n",
    "        # for future in tqdm(as_completed(futures), total=len(df), desc=\"Processing Rows\"):\n",
    "        #     chunks_list.extend(future.result())\n",
    "        print(\"in the future\")\n",
    "        # pbar = tqdm(total=len(df)//batch_size, desc=\"Processing Rows\")\n",
    "        for future in as_completed(futures):\n",
    "            chunks_list.extend(future.result())\n",
    "            # print(len(chunks_list))\n",
    "            # pbar.update(1)  # Manually update the progress bar\n",
    "        pbar.close()\n",
    "    return chunks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the DataFrame and create a new DataFrame from the list of chunks\n",
    "start = time.perf_counter()\n",
    "print(f\"Chunking text that is longer than {MAX_TOKENS} tokens\")\n",
    "chunked_data = process_dataframe(df)\n",
    "print(f\"Dataset chunked in {time.perf_counter() - start:.2f} seconds\")\n",
    "start = time.perf_counter()\n",
    "chunked_df = pd.DataFrame(chunked_data)\n",
    "print(f\"Dataset converted to DataFrame in {time.perf_counter() - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunked_df.to_parquet(\"chunked-\" + file.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
